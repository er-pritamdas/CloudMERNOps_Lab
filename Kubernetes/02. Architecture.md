# 🌐 Kubernetes Architecture Overview

Kubernetes is a  **container orchestration system** . Think of it as an operating system for your cluster.

It manages your **applications (containers)** and ensures they run as expected on a set of  **machines (nodes)** .

At the highest level, Kubernetes architecture has  **two planes** :

1. **Control Plane** → The “brain” (management layer).
   * Decides *what* should run, *where* it should run, and *how* the cluster behaves.
   * Runs on a **Master Node** (or multiple for HA).
2. **Data Plane / Worker Nodes** → The “muscles” (execution layer).
   * Actually run the containers (your applications).
   * Worker nodes execute the instructions given by the control plane.

---

# 🧠 Control Plane Components (Master)

The control plane is made up of several components that together provide cluster management:

### 1. **API Server (`kube-apiserver`)**

* The **front door** to Kubernetes.
* Every interaction (kubectl, dashboards, other services) goes through the API server.
* It exposes a  **REST API** .
* Example: When you type `kubectl apply -f deployment.yaml`, it talks to the API Server.

---

### 2. **etcd**

* A **key-value store** that holds all cluster data (the “source of truth”).
* Stores:
  * Nodes in the cluster
  * Configurations
  * Secrets
  * Workloads, replicas, policies
* Highly available and consistent.
* Think of it as the  **Kubernetes database** .

---

### 3. **Controller Manager (`kube-controller-manager`)**

* Runs **controllers** (background processes) that constantly check the state of the cluster and make corrections.
* Examples:
  * **Node Controller** → ensures nodes are healthy.
  * **Replication Controller** → ensures the right number of pods are running.
  * **Job Controller** → monitors jobs and pods.
* Analogy: It’s like a **thermostat** → if desired state ≠ current state, it fixes it.

---

### 4. **Scheduler (`kube-scheduler`)**

* Decides **where pods should run** (which worker node).
* Uses constraints:
  * Resource availability (CPU, memory)
  * Node affinity/anti-affinity
  * Taints and tolerations
* Example: If a new pod needs to be scheduled, the scheduler finds the best worker node.

---

### 5. **Cloud Controller Manager** (optional, cloud integrations)

* If running in AWS, GCP, Azure → interacts with their APIs.
* Handles things like:
  * Load Balancers
  * Storage Volumes
  * Node provisioning

---

# ⚙️ Worker Node Components (Data Plane)

Each worker node is responsible for running your **pods** (containers). Key components:

### 1. **Kubelet**

* Agent that runs on every node.
* Talks to the API server.
* Ensures:
  * Pods described in etcd actually run on the node.
  * Containers are healthy.
* Think of it as a  **node manager** .

---

### 2. **Kube-proxy**

* Handles **networking** on each node.
* Implements **Kubernetes Services** (ClusterIP, NodePort, LoadBalancer).
* Uses iptables or IPVS to forward traffic correctly.

---

### 3. **Container Runtime**

* Actual engine that runs containers.
* Examples: Docker, containerd, CRI-O.
* Kubernetes doesn’t run containers itself → it asks the container runtime to do so.

---

# 🧩 Pods & Higher-Level Abstractions

* **Pod** → Smallest deployable unit in Kubernetes. A pod wraps one or more containers.
* **ReplicaSet** → Ensures multiple copies of pods exist.
* **Deployment** → Manages ReplicaSets and rolling updates.
* **Service** → Provides stable networking to pods.
* **ConfigMap & Secret** → Store configuration and sensitive data.
* **Ingress** → Manages external access (usually HTTP/HTTPS).

---

# 🔄 How it All Works Together (Flow)

1. You run `kubectl apply -f deployment.yaml`.
2. **kubectl** → talks to  **API Server** .
3. **API Server** → stores desired state in  **etcd** .
4. **Controller Manager** → notices a new Deployment.
5. **Scheduler** → picks the best worker node.
6. **Kubelet** on that node → tells container runtime (Docker/containerd) to start the container.
7. **Kube-proxy** → configures networking so traffic can reach the pod.
8. Controllers keep monitoring → If a pod dies, a new one is started.

---



# 📝 Ways of Running Kubernetes

---

## 1. **Local Development Environments**

Best for **learning & experimenting** on a single machine (laptop, VM, or EC2).

### 🔹 Minikube

* Runs K8s cluster **inside a VM or a big container** on your host.
* By default, master + worker are inside that environment.
* Good for beginners, lightweight clusters, testing.
* Driver options: Docker, VirtualBox, Hyper-V.
* Pods are  **containers inside the big container/VM** .

  ✅ Pros: Easy to install, simple for beginners.

  ❌ Cons: Nested container setup, not like real production.

---

### 🔹 Kind (Kubernetes-in-Docker)

* Each Kubernetes node (master/worker) is a  **Docker container** .
* Pods run inside those containerized nodes.
* Great for CI/CD testing because clusters start fast.

  ✅ Pros: Super fast, lightweight, easy for pipelines.

  ❌ Cons: Still “containers inside containers,” not real-world infra.

---

### 🔹 K3s

* Lightweight Kubernetes (by Rancher/SUSE).
* Installs directly on the host (no big container).
* Uses **containerd** by default (no heavy Docker).
* Great for edge devices, Raspberry Pi, IoT.

  ✅ Pros: Tiny footprint, production-ready for small/edge infra.

  ❌ Cons: Not full enterprise K8s (but very close).

---

### 🔹 MicroK8s

* Lightweight distribution from Canonical (Ubuntu).
* Runs on host machine, minimal dependencies.
* Can easily enable/disable features with `microk8s enable ...`.
* Production-ready for both small and big environments.

  ✅ Pros: Easy single command setup, close to real K8s.

  ❌ Cons: Mostly Ubuntu-focused.

---

## 2. **Production-Style DIY Setup**

Best for  **serious labs or real-world deployments** .

### 🔹 kubeadm

* Official tool from Kubernetes project.
* You provision  **your own machines (VMs/EC2/bare metal)** .
* On master node → install control plane (apiserver, etcd, scheduler).
* On worker nodes → install kubelet + join cluster.
* Pods run **directly on host’s runtime** (Docker/containerd).

  ✅ Pros: Real production-style, flexible, teaches internals.

  ❌ Cons: Manual setup (you manage everything).

---

### 🔹 Kubespray

* Ansible-based automation to set up Kubernetes across machines.
* Uses kubeadm under the hood but automated.
* Good for bigger clusters.

  ✅ Pros: Less manual than kubeadm.

  ❌ Cons: More complexity for small labs.

---

### 🔹 Rancher

* GUI + automation platform for managing K8s clusters.
* Can deploy & manage multiple clusters (on-prem, cloud, hybrid).

  ✅ Pros: Enterprise features, multi-cluster management.

  ❌ Cons: More overhead than you need for just learning.

---

## 3. **Managed Kubernetes (Cloud Providers)**

Best for **production & enterprises** who don’t want to manage infra.

### 🔹 AWS EKS (Elastic Kubernetes Service)

* AWS manages the master (control plane).
* You manage worker nodes (or use Fargate for serverless pods).

### 🔹 Google GKE (Google Kubernetes Engine)

* Google pioneered K8s, so GKE is most mature.
* Control plane fully managed.

### 🔹 Azure AKS (Azure Kubernetes Service)

* Microsoft’s managed service.

✅ Pros: No headache of managing masters, high availability, autoscaling, cloud integrations.

❌ Cons: Costs $, less control over low-level configs.

---

# 🏁 Summary

* **Minikube / Kind** → Best for quick  **local learning** , but not production-like.
* **MicroK8s / K3s** → Lightweight, good for  **host-based single-node clusters** .
* **kubeadm** → Best for **realistic production-style learning** (separate master/worker).
* **Managed K8s (EKS/GKE/AKS)** → For **enterprise production** clusters in cloud.

---

👉 Think of it like  **levels of learning** :

1. Start with **Minikube/Kind** (easy intro).
2. Move to **MicroK8s/K3s** (pods on host directly).
3. Graduate to **kubeadm with multiple EC2s** (real cluster).
4. Finally, explore  **cloud-managed K8s (EKS/GKE/AKS)** .

---
